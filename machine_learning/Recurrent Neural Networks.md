# Recurrent Neural Networks (RNNs)

Recurrent Neural Networks (RNNs) are a class of neural networks designed to process sequential data. Unlike traditional feedforward neural networks, RNNs have a "memory" that allows them to use information from previous steps in a sequence to influence the current output. This makes them particularly well-suited for tasks involving time series, natural language processing, and speech recognition.

## 1. Intuitive Explanation: The "Memory" of a Neural Network

Imagine you're reading a sentence. To understand the meaning of a word, you often need to consider the words that came before it. For example, in the sentence "I saw a **bank**," the meaning of "bank" (river bank vs. financial institution) depends on the surrounding context.

Traditional neural networks treat each input independently. If you feed them words one by one, they'd forget the previous words, making it hard to understand the full sentence.

RNNs solve this by introducing a "recurrent" connection. Think of it like this:

*   **Each step in the sequence:** An RNN processes one item (e.g., one word) at a time.
*   **Internal state (memory):** After processing an item, the RNN updates an internal "state" or "memory" that encapsulates what it has learned so far from the sequence.
*   **Passing the memory:** This updated memory is then passed on to the next step in the sequence.
*   **Influence on output:** The current input, combined with the previous memory, determines the current output and the new memory state.

This allows RNNs to learn patterns and dependencies across time or sequence steps, making them powerful for tasks where context is crucial.

## 2. Visual Representation

Let's visualize the basic structure of an RNN.

### Unrolled RNN

A common way to understand RNNs is by "unrolling" them over time. This shows how the network processes each step in a sequence.

Input: x_0 --> x_1 --> x_2 --> ... --> x_t  
| | | |  
v v v v  
Hidden: h_0 --> h_1 --> h_2 --> ... --> h_t  
| | | |  
v v v v  
Output: o_0 --> o_1 --> o_2 --> ... --> o_t

In this diagram:

*   $x_t$ |: Input at time step $t$.
*   $h_t$ |: Hidden state (memory) at time step $t$. This is the "recurrent" part, as it's passed from one step to the next.
*   $o_t$ |: Output at time step $t$.
*   $U, W, V$ |: Weight matrices that are shared across all time steps. This is a crucial aspect of RNNs â€“ the same transformation is applied at each step, but with different inputs and hidden states.

### Rolled RNN

The rolled representation shows the recurrent connection more compactly:
+-----+
x_t -->| RNN |--> o_t  
| |  
+--^--+  
|  
+----(h_t-1)

Here, the loop indicates that the hidden state from the previous time step is fed back into the network at the current time step.

## 3. Mathematical Notation

The core equations for a simple RNN are as follows:

**Hidden State Update:**
$h_t = f(W_{hh} h_{t-1} + W_{xh} x_t + b_h)$

**Output Calculation:**
$o_t = g(W_{ho} h_t + b_o)$

Where:

*   $x_t$ |: Input vector at time step $t$.
*   $h_t$ |: Hidden state vector at time step $t$.
*   $h_{t-1}$ |: Hidden state vector from the previous time step $t-1$.
*   $o_t$ |: Output vector at time step $t$.
*   $W_{hh}$ |: Weight matrix for the recurrent connection (hidden-to-hidden).
*   $W_{xh}$ |: Weight matrix for the input-to-hidden connection.
*   $W_{ho}$ |: Weight matrix for the hidden-to-output connection.
*   $b_h$ |: Bias vector for the hidden layer.
*   $b_o$ |: Bias vector for the output layer.
*   $f$ |: Activation function for the hidden layer (e.g., tanh, ReLU).
*   $g$ |: Activation function for the output layer (e.g., softmax for classification, linear for regression).

**Key point:** The weight matrices ($W_{hh}, W_{xh}, W_{ho}$) and bias vectors ($b_h, b_o$) are **shared** across all time steps. This is what allows the RNN to learn general patterns that apply throughout the sequence.

## 4. Different Types of RNNs

While the basic RNN is a foundational concept, several variations have been developed to address its limitations, particularly the vanishing/exploding gradient problem and difficulty in capturing long-range dependencies.

### a. One-to-One (Vanilla Neural Network)

*   **Description:** This is a standard feedforward neural network where there's one input and one output, with no recurrent connections.
*   **Use Cases:** Image classification, simple regression tasks.

### b. One-to-Many

*   **Description:** Takes a single input and produces a sequence of outputs.
*   **Use Cases:** Image captioning (input: image, output: sequence of words describing the image), music generation (input: starting note/seed, output: sequence of notes).

### c. Many-to-One

*   **Description:** Takes a sequence of inputs and produces a single output.
*   **Use Cases:** Sentiment analysis (input: sequence of words in a review, output: positive/negative sentiment), spam detection (input: email text, output: spam/not spam).

### d. Many-to-Many (Sequence-to-Sequence)

This category can be further divided:

#### i. Many-to-Many (Same Length)

*   **Description:** Takes a sequence of inputs and produces a sequence of outputs of the same length.
*   **Use Cases:** Video classification (frame-by-frame classification), named entity recognition (input: sequence of words, output: sequence of labels indicating entities).

#### ii. Many-to-Many (Different Lengths - Encoder-Decoder)

*   **Description:** This is a more complex architecture often used for tasks where the input and output sequences have different lengths. It consists of an **encoder** RNN that processes the input sequence and compresses it into a fixed-size context vector, and a **decoder** RNN that takes this context vector and generates the output sequence.
*   **Use Cases:** Machine translation (input: sentence in one language, output: sentence in another language), speech recognition (input: audio sequence, output: text sequence).

### e. Long Short-Term Memory (LSTM) Networks

*   **Description:** LSTMs are a special kind of RNN designed to overcome the vanishing gradient problem and better capture long-term dependencies. They achieve this through a more complex internal structure called a "cell state" and various "gates" (input, forget, output gates) that control the flow of information into and out of the cell state.
*   **Advantages:** Excellent at remembering information for extended periods.
*   **Use Cases:** Speech recognition, machine translation, language modeling, time series prediction.

### f. Gated Recurrent Units (GRUs)

*   **Description:** GRUs are a simplified version of LSTMs. They also use gating mechanisms (reset gate and update gate) to control information flow, but they have fewer parameters than LSTMs, making them computationally less expensive and sometimes faster to train.
*   **Advantages:** Often perform similarly to LSTMs on many tasks, but with less complexity.
*   **Use Cases:** Similar to LSTMs, especially when computational resources are a concern.

RNNs, particularly LSTMs and GRUs, have revolutionized many areas of AI by enabling models to understand and generate sequential data with remarkable accuracy.